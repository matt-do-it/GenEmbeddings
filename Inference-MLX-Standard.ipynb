{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7ad32040-0c28-4683-bf79-38f5c6a66ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\"./models/gemma3-1b-it/transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c35351a0-d995-455e-878b-2613f775649f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 818, 2862, 2436, 795, 817, 872, 528, 1390]\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.encode(\"The stock market will go up in...\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e51c8015-9a38-4502-aa64-19396236021c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "\n",
      "\n",
      "...because people are hungry for growth.\n",
      "\n",
      "More money on the wait!\n",
      "<end_of_turn><end_of_turn><end_of_turn>\n",
      "==========\n",
      "Prompt: 8 tokens, 75.055 tokens-per-sec\n",
      "Generation: 20 tokens, 41.144 tokens-per-sec\n",
      "Peak memory: 4.000 GB\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm.sample_utils import make_sampler\n",
    "sampler = make_sampler(1.0, top_p=2)\n",
    "\n",
    "text = generate(model, tokenizer, prompt=\"The stock market will go up...\", sampler=sampler, verbose=True, max_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1002a49f-1f35-4d45-b7fd-70f7baff0597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "\n",
      "\n",
      "**June.**\n",
      "\n",
      "Another interesting move.\n",
      "\n",
      "Would you like to know more about this?\n",
      "\n",
      "==========\n",
      "Prompt: 9 tokens, 134.160 tokens-per-sec\n",
      "Generation: 20 tokens, 42.487 tokens-per-sec\n",
      "Peak memory: 4.000 GB\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm.sample_utils import make_sampler\n",
    "sampler = make_sampler(1.0)\n",
    "\n",
    "\n",
    "text = generate(model, tokenizer, prompt=prompt, sampler=sampler, verbose=True, max_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5cf594e0-d5a4-4971-a61a-c5d828f012ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 105, 2364, 107, 3048, 735, 2802, 531, 5151, 236761, 1637, 611, 10741, 531, 55899, 1027, 529, 506, 1292, 236769, 236751, 779, 107, 611, 59931, 2247, 625, 528, 506, 6518, 529, 107, 236840, 6823, 236779, 1201, 236770, 236769, 6160, 236779, 1201, 236770, 236784, 6160, 236779, 2394, 236770, 236764, 9388, 236779, 1201, 236778, 236784, 6160, 236779, 2394, 236778, 121825, 6051, 236779, 1201, 236778, 236769, 6160, 7066, 108, 3048, 96215, 5244, 3204, 1027, 1032, 1816, 528, 506, 3072, 768, 611, 2246, 496, 1292, 107, 236840, 107, 138, 236782, 107, 140, 236775, 1201, 1083, 623, 828, 236779, 5930, 236779, 1201, 236779, 2003, 236779, 105300, 827, 107, 140, 236775, 7777, 1083, 623, 11823, 236751, 506, 1463, 529, 496, 1698, 684, 1061, 7303, 5853, 827, 107, 140, 236775, 19031, 1083, 642, 107, 142, 236775, 2084, 1083, 623, 5973, 827, 107, 142, 236775, 15921, 1083, 642, 107, 144, 236775, 105300, 1083, 642, 107, 146, 236775, 2084, 1083, 623, 2383, 236775, 107, 144, 236783, 107, 142, 1263, 107, 142, 236775, 15979, 1083, 870, 107, 144, 236775, 105300, 236775, 107, 142, 236842, 107, 140, 236783, 107, 138, 236783, 107, 236842, 109, 818, 2862, 2436, 795, 817, 872, 528, 1390, 106, 107, 105, 4368, 107]\n",
      "<bos><start_of_turn>user\n",
      "You have access to functions. If you decide to invoke any of the function(s),\n",
      " you MUST put it in the format of\n",
      "[func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n",
      "\n",
      "You SHOULD NOT include any other text in the response if you call a function\n",
      "[\n",
      "  {\n",
      "    \"name\": \"get_product_name_by_PID\",\n",
      "    \"description\": \"Finds the name of a product by its Product ID\",\n",
      "    \"parameters\": {\n",
      "      \"type\": \"object\",\n",
      "      \"properties\": {\n",
      "        \"PID\": {\n",
      "          \"type\": \"string\"\n",
      "        }\n",
      "      },\n",
      "      \"required\": [\n",
      "        \"PID\"\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "]\n",
      "\n",
      "\n",
      "The stock market will go up in...<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# User turn\n",
    "user = \"The stock market will go up in...\"\n",
    "\n",
    "# An example tool, make sure to include a docstring and type hints\n",
    "def multiply(a: float, b: float):\n",
    "    \"\"\"\n",
    "    A function that multiplies two numbers\n",
    "\n",
    "    Args:\n",
    "        a: The first number to multiply\n",
    "        b: The second number to multiply\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "tools = {\"multiply\": multiply}\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \n",
    "         [{\"text\": \"\"\"You have access to functions. If you decide to invoke any of the function(s),\n",
    " you MUST put it in the format of\n",
    "[func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n",
    "\n",
    "You SHOULD NOT include any other text in the response if you call a function\n",
    "[\n",
    "  {\n",
    "    \"name\": \"get_product_name_by_PID\",\n",
    "    \"description\": \"Finds the name of a product by its Product ID\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"PID\": {\n",
    "          \"type\": \"string\"\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\n",
    "        \"PID\"\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "]\n",
    "\"\"\"}]},\n",
    "    {\"role\": \"user\", \"content\": user}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tools = (multiply,))\n",
    "\n",
    "print(prompt)\n",
    "print(tokenizer.decode(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3eb9510f-7c90-44d9-beec-78ee45592a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"name\": \"get_product_name_by_PID\",\n",
      "    \"description\": \"Finds the name of a product by its Product ID\",\n",
      "    \"parameters\": {\n",
      "      \"type\": \"object\",\n",
      "      \"properties\": {\n",
      "        \"PID\": {\n",
      "          \"type\": \"string\"\n",
      "        }\n",
      "      },\n",
      "      \"required\": [\n",
      "        \"PID\"\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "]\n",
      "```<end_of_turn><end_of_turn>'The stock market will go up in...'<end_of_turn>\n",
      "<end_of_turn>ی\n",
      "<end_of_turn>ی\n",
      "<end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn><end_of_turn>\n",
      "==========\n",
      "Prompt: 196 tokens, 740.685 tokens-per-sec\n",
      "Generation: 200 tokens, 40.456 tokens-per-sec\n",
      "Peak memory: 2.312 GB\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm.sample_utils import make_sampler\n",
    "sampler = make_sampler(1.0)\n",
    "\n",
    "text = generate(model, tokenizer, prompt=prompt, verbose=True, max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7e2e34f9-de6e-4f3e-a35b-227a955c0580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3048, 735, 2802, 531, 5151, 236761, 1637, 611, 10741, 531, 55899, 1027, 529, 506, 1292, 236769, 236751, 779, 107, 7624, 59931, 2247, 625, 528, 506, 6518, 529, 107, 14937, 1201, 1083, 1292, 1463, 236764, 623, 19031, 1083, 19086, 529, 8485, 1463, 532, 1061, 1550, 236783, 108, 3048, 96215, 5244, 3204, 1027, 1032, 1816, 528, 506, 3072, 768, 611, 2246, 496, 1292, 107, 236840, 107, 138, 236782, 107, 140, 236775, 1201, 1083, 623, 828, 236779, 5930, 236779, 1201, 236779, 2003, 236779, 105300, 827, 107, 140, 236775, 7777, 1083, 623, 11823, 236751, 506, 1463, 529, 496, 1698, 684, 1061, 7303, 5853, 827, 107, 140, 236775, 19031, 1083, 642, 107, 142, 236775, 2084, 1083, 623, 5973, 827, 107, 142, 236775, 15921, 1083, 642, 107, 144, 236775, 105300, 1083, 642, 107, 146, 236775, 2084, 1083, 623, 2383, 236775, 107, 144, 236783, 107, 142, 1263, 107, 142, 236775, 15979, 1083, 870, 107, 144, 236775, 105300, 236775, 107, 142, 236842, 107, 140, 236783, 107, 138, 236783, 107, 236842, 107, 8409, 38101, 506, 1698, 23191, 236764, 564, 3588, 3418, 496, 1698, 600, 166632, 524, 1041, 107, 21878, 236761, 669, 1698, 5853, 563, 236743, 236828, 236771, 236832, 236953, 32436, 8017, 236819, 236847, 236761, 3199, 611, 1601, 786, 1586, 506, 1463, 529, 672, 107, 5930, 236881, 107]\n",
      "<bos>You have access to functions. If you decide to invoke any of the function(s),\n",
      "you MUST put it in the format of\n",
      "{\"name\": function name, \"parameters\": dictionary of argument name and its value}\n",
      "\n",
      "You SHOULD NOT include any other text in the response if you call a function\n",
      "[\n",
      "  {\n",
      "    \"name\": \"get_product_name_by_PID\",\n",
      "    \"description\": \"Finds the name of a product by its Product ID\",\n",
      "    \"parameters\": {\n",
      "      \"type\": \"object\",\n",
      "      \"properties\": {\n",
      "        \"PID\": {\n",
      "          \"type\": \"string\"\n",
      "        }\n",
      "      },\n",
      "      \"required\": [\n",
      "        \"PID\"\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "]\n",
      "While browsing the product catalog, I came across a product that piqued my\n",
      "interest. The product ID is 807ZPKBL9V. Can you help me find the name of this\n",
      "product?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.encode(\"\"\"You have access to functions. If you decide to invoke any of the function(s),\n",
    "you MUST put it in the format of\n",
    "{\"name\": function name, \"parameters\": dictionary of argument name and its value}\n",
    "\n",
    "You SHOULD NOT include any other text in the response if you call a function\n",
    "[\n",
    "  {\n",
    "    \"name\": \"get_product_name_by_PID\",\n",
    "    \"description\": \"Finds the name of a product by its Product ID\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"PID\": {\n",
    "          \"type\": \"string\"\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\n",
    "        \"PID\"\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "]\n",
    "While browsing the product catalog, I came across a product that piqued my\n",
    "interest. The product ID is 807ZPKBL9V. Can you help me find the name of this\n",
    "product?\n",
    "\"\"\")\n",
    "\n",
    "print(prompt)\n",
    "print(tokenizer.decode(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "780b751f-31c3-4fbd-88fb-3969e679aa5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "```json\n",
      "{\"name\": \"get_product_name_by_PID\", \"parameters\": { \"PID\": \"807ZPKBL9V\" }}\n",
      "```\n",
      "```json\n",
      "{\"name\": \"get_product_name_by_PID\", \"parameters\": { \"PID\": \"807ZPKBL9V\" }}\n",
      "```\n",
      "```json\n",
      "{\"name\": \"get_product_name_by_PID\", \"parameters\": { \"PID\": \"807ZPKBL9V\" }}\n",
      "```\n",
      "```json\n",
      "{\"name\": \"get_product_name_by_PID\", \"parameters\": { \"PID\": \"807ZPKBL9V\" }}\n",
      "```\n",
      "```json\n",
      "{\"name\": \"get_product_name_by_PID\", \"parameters\": { \"PID\": \"807ZPKBL9V\" }}\n",
      "```\n",
      "```json\n",
      "{\"name\": \"get_product\n",
      "==========\n",
      "Prompt: 209 tokens, 806.063 tokens-per-sec\n",
      "Generation: 200 tokens, 40.282 tokens-per-sec\n",
      "Peak memory: 2.312 GB\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm.sample_utils import make_sampler\n",
    "sampler = make_sampler(1.0)\n",
    "\n",
    "text = generate(model, tokenizer, prompt=prompt, verbose=True, max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60912ed7-10ab-428c-b3a6-89d10103d7ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma",
   "language": "python",
   "name": "gemma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
