{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90aedaa-d184-453b-b416-6613f4f1961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import re\n",
    "import types\n",
    "from pathlib import Path\n",
    "\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "import numpy as np\n",
    "\n",
    "from mlx_lm.tuner.datasets import load_dataset\n",
    "from mlx_lm.tuner.trainer import TrainingArgs, TrainingCallback, evaluate, train\n",
    "from mlx_lm.tuner.utils import (\n",
    "    build_schedule,\n",
    "    linear_to_lora_layers,\n",
    "    load_adapters,\n",
    "    print_trainable_parameters,\n",
    ")\n",
    "from mlx_lm.utils import load, save_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e46837-c152-402d-a586-4cf40dc92616",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"model\": \"./models/gemma3-1b-it/transformers\",\n",
    "    \"train\": False,\n",
    "    \"fine_tune_type\": \"lora\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"optimizer_config\": {\n",
    "        \"adam\": {},\n",
    "        \"adamw\": {},\n",
    "    },\n",
    "    \"data\": \"./training/\",\n",
    "    \"seed\": 0,\n",
    "    \"num_layers\": 16,\n",
    "    \"batch_size\": 1,\n",
    "    \"iters\": 1000,\n",
    "    \"val_batches\": 25,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"steps_per_report\": 10,\n",
    "    \"steps_per_eval\": 200,\n",
    "    \"resume_adapter_file\": None,\n",
    "    \"adapter_path\": \"adapters\",\n",
    "    \"save_every\": 100,\n",
    "    \"test\": False,\n",
    "    \"test_batches\": 500,\n",
    "    \"max_seq_length\": 2048,\n",
    "    \"config\": None,\n",
    "    \"grad_checkpoint\": False,\n",
    "    \"lr_schedule\": None,\n",
    "    \"lora_parameters\": {\"rank\": 8, \"dropout\": 0.0, \"scale\": 10.0},\n",
    "    \"mask_prompt\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8249be3-361e-4b51-9656-7ad02e6f0524",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = types.SimpleNamespace(**options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7263b51-549e-4a55-b89c-8ff1b839819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading pretrained model\")\n",
    "model, tokenizer = load(args.model)\n",
    "\n",
    "print(\"Loading datasets\")\n",
    "train_set, valid_set, test_set = load_dataset(args, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31022436-f986-4ac1-97ee-c1e727237431",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_callback = None\n",
    "\n",
    "mx.random.seed(args.seed)\n",
    "model.freeze()\n",
    "if args.num_layers > len(model.layers):\n",
    "    raise ValueError(\n",
    "        f\"Requested to train {args.num_layers} layers \"\n",
    "        f\"but the model only has {len(model.layers)} layers.\"\n",
    "    )\n",
    "\n",
    "if args.fine_tune_type == \"full\":\n",
    "    for l in model.layers[-max(args.num_layers, 0) :]:\n",
    "        l.unfreeze()\n",
    "elif args.fine_tune_type in [\"lora\", \"dora\"]:\n",
    "    # Convert linear layers to lora/dora layers and unfreeze in the process\n",
    "    linear_to_lora_layers(\n",
    "        model,\n",
    "        args.num_layers,\n",
    "        args.lora_parameters,\n",
    "        use_dora=(args.fine_tune_type == \"dora\"),\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Received unknown fine-tune-type {args.fine_tune_type}\")\n",
    "\n",
    "# Resume from weights if provided\n",
    "if args.resume_adapter_file is not None:\n",
    "    print(f\"Loading fine-tuned weights from {args.resume_adapter_file}\")\n",
    "    model.load_weights(args.resume_adapter_file, strict=False)\n",
    "\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "adapter_path = Path(args.adapter_path)\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "adapter_file = adapter_path / \"adapters.safetensors\"\n",
    "save_config(vars(args), adapter_path / \"adapter_config.json\")\n",
    "\n",
    "# init training args\n",
    "training_args = TrainingArgs(\n",
    "    batch_size=args.batch_size,\n",
    "    iters=args.iters,\n",
    "    val_batches=args.val_batches,\n",
    "    steps_per_report=args.steps_per_report,\n",
    "    steps_per_eval=args.steps_per_eval,\n",
    "    steps_per_save=args.save_every,\n",
    "    adapter_file=adapter_file,\n",
    "    max_seq_length=args.max_seq_length,\n",
    "    grad_checkpoint=args.grad_checkpoint,\n",
    ")\n",
    "\n",
    "# Initialize the selected optimizer\n",
    "lr = build_schedule(args.lr_schedule) if args.lr_schedule else args.learning_rate\n",
    "\n",
    "optimizer_name = args.optimizer.lower()\n",
    "optimizer_config = args.optimizer_config.get(optimizer_name, {})\n",
    "\n",
    "if optimizer_name == \"adam\":\n",
    "    opt_class = optim.Adam\n",
    "elif optimizer_name == \"adamw\":\n",
    "    opt_class = optim.AdamW\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "\n",
    "opt = opt_class(learning_rate=lr, **optimizer_config)\n",
    "\n",
    "# Train model\n",
    "train(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    optimizer=opt,\n",
    "    train_dataset=train_set,\n",
    "    val_dataset=valid_set,\n",
    "    training_callback=training_callback,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma",
   "language": "python",
   "name": "gemma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
