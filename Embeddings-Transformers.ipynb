{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b14f63-03eb-46c6-aaa3-2d273848c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "sentences = [\n",
    "\t\"How did the stock market develop?\",\n",
    "\t\"What is the development of the stock market?\",\n",
    "\t\"How can I have great organic food?\",\n",
    "\t\"The stock did drop.\",\n",
    "\t\"The stock price did fall.\",\n",
    "\t\"Price of the stock did reduce.\", \n",
    "\t\"The price of this stock did go up.\",\n",
    "\t\"Der Kurs dieser Aktie ist gestiegen.\"\n",
    "]\n",
    "\n",
    "def gen_embeddings(model_name, sentences):\n",
    "    # --- Setup the model ---\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "    all_embeddings = []\n",
    "    \n",
    "    # --- Generate embeddings ---\n",
    "    for sentence in sentences:\n",
    "        encoded = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            output = model(**encoded)\n",
    "    \n",
    "        # Mean pooling\n",
    "        token_embeddings = output.last_hidden_state\n",
    "        attention_mask = encoded['attention_mask']\n",
    "    \n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size())\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        sentence_embedding = sum_embeddings / sum_mask\n",
    "        print(sentence_embedding)\n",
    "    \n",
    "    #    sentence_embedding = output.last_hidden_state[:, 0, :]\n",
    "    \n",
    "        all_embeddings.append(sentence_embedding.squeeze(0))  # remove batch dim\n",
    "    \n",
    "    embedding_tensor = torch.stack(all_embeddings)\n",
    "    return embedding_tensor\n",
    "    \n",
    "def write_tensorboard(embedding_tensor, out_name):\n",
    "    # 5. Write to TensorBoard\n",
    "    # --- Initialize tensorboard ---\n",
    "    log_dir = \"runs/\" + out_name\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    writer.add_embedding(embedding_tensor, metadata=sentences)\n",
    "    writer.close()\n",
    "        \n",
    "        \n",
    "out1 = gen_embeddings(\"sentence-transformers/all-MiniLM-L6-v2\", sentences)\n",
    "write_tensorboard(out1, \"all-MiniLM-L6-v2\")\n",
    "\n",
    "out2 = gen_embeddings(\"distilbert/distilbert-base-uncased\", sentences)\n",
    "write_tensorboard(out2, \"distilbert-base-uncased\")\n",
    "\n",
    "out3 = gen_embeddings(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", sentences)\n",
    "write_tensorboard(out3, \"multilingual\")\n",
    "\n",
    "\n",
    "## Generate vega embeddings \n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "results_1 = pca.fit_transform(out1)\n",
    "results_2 = pca.fit_transform(out2)\n",
    "results_3 = pca.fit_transform(out3)\n",
    "    \n",
    "df1 = pd.DataFrame(results_1, columns=['x', 'y'])\n",
    "df1['label'] = sentences\n",
    "df1['Model'] = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "df2 = pd.DataFrame(results_2, columns=['x', 'y'])\n",
    "df2['label'] = sentences\n",
    "df2['Model'] = \"distilbert-base-uncased\"\n",
    "\n",
    "df3 = pd.DataFrame(results_3, columns=['x', 'y'])\n",
    "df3['label'] = sentences\n",
    "df3['Model'] = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "combined = pd.concat([df3], ignore_index=True)\n",
    "\n",
    "vega_spec = {\n",
    "    \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n",
    "    \"description\": \"t-SNE visualization of sentence embeddings\",\n",
    "    \"data\": {\n",
    "        \"values\": combined.to_dict(orient=\"records\")\n",
    "    },\n",
    "    \"mark\": \"point\",\n",
    "    \"encoding\": {\n",
    "        \"x\": {\"field\": \"x\", \"type\": \"quantitative\"},\n",
    "        \"y\": {\"field\": \"y\", \"type\": \"quantitative\"},\n",
    "        \"color\": {\"field\": \"Model\", \"type\": \"nominal\"},\n",
    "        \"tooltip\": [{\"field\": \"label\"}]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Optionally save to file\n",
    "with open(\"vega.json\", \"w\") as f:\n",
    "    json.dump(vega_spec, f, indent=2)\n",
    "\n",
    "# Normalize embeddings to unit vectors\n",
    "normalized = F.normalize(out3, p=2, dim=1)  # Still shape (5, 768)\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim_matrix = torch.matmul(normalized, normalized.T)  # Shape: (5, 5)\n",
    "similarities = cosine_similarity(out3)\n",
    "print(cosine_sim_matrix)\n",
    "\n",
    "# Create DataFrame and flatten it\n",
    "df = pd.DataFrame(similarities, index=sentences, columns=sentences)\n",
    "df_reset = df.reset_index().melt(id_vars='index')\n",
    "df_reset.columns = ['y', 'x', 'similarity']\n",
    "\n",
    "# Convert to list of dictionaries\n",
    "vega_data = df_reset.to_dict(orient='records')\n",
    "\n",
    "# Vega-Lite spec\n",
    "vega_spec = {\n",
    "    \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n",
    "    \"description\": \"Cosine similarity heatmap for sentences\",\n",
    "    \"data\": {\n",
    "        \"values\": vega_data\n",
    "    },\n",
    "    \"mark\": \"rect\",\n",
    "    \"width\": 400,\n",
    "    \"height\": 400,\n",
    "    \"encoding\": {\n",
    "        \"x\": {\n",
    "            \"field\": \"x\",\n",
    "            \"type\": \"nominal\",\n",
    "            \"axis\": {\"labelAngle\": 45, \"title\": \"Sentence X\"}\n",
    "        },\n",
    "        \"y\": {\n",
    "            \"field\": \"y\",\n",
    "            \"type\": \"nominal\",\n",
    "            \"axis\": {\"title\": \"Sentence Y\"}\n",
    "        },\n",
    "        \"color\": {\n",
    "            \"field\": \"similarity\",\n",
    "            \"type\": \"quantitative\",\n",
    "            \"scale\": {\"scheme\": \"blues\"},\n",
    "            \"legend\": {\"title\": \"Cosine Similarity\"}\n",
    "        },\n",
    "        \"tooltip\": [\n",
    "            {\"field\": \"x\", \"type\": \"nominal\"},\n",
    "            {\"field\": \"y\", \"type\": \"nominal\"},\n",
    "            {\"field\": \"similarity\", \"type\": \"quantitative\", \"format\": \".2f\"}\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "with open('cosine_similarity_vega.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(vega_spec, f, indent=2)\n",
    "\n",
    "print(\"Vega spec saved to 'cosine_similarity_vega.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112047dd-f973-4638-8a55-706cc03ddcbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma",
   "language": "python",
   "name": "gemma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
